# -*- coding: utf-8 -*-
"""Resnet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MlvhSFZrHeRfa5Lzi4Hdyr4UwqFSGDni

https://www.notion.so/yash-sharma/Cross-modal-representation-0a065f6a9e804756803ee1d287eaa73e
"""

import torch
import torch.nn as nn
import torch.nn.functional as F 
import torchvision
import torchvision.transforms as transforms
import os
import pandas as pd

from torch.utils.data import Dataset
from torchvision import models
from PIL import Image
from torchvision import datasets
from torchvision.io import read_image
from torch.utils.data import DataLoader

#dir(models)
from itertools import islice
import tarfile
import pickle
from tqdm import tqdm

resnet = models.resnet50(pretrained=True)

resnet.cuda()

# CUDA for PyTorch
use_cuda = torch.cuda.is_available()
device = torch.device("cuda:0" if use_cuda else "cpu")
torch.backends.cudnn.benchmark = True

for param in resnet.parameters():
  param.requires_grad = False
  #print(param)

"""# Mount drive"""

from google.colab import drive
drive.mount('/content/drive')

"""# Feature extraction"""

def get_feature_vector(img_name):
  print("Hello")
  #img = img.convert("RGB")
  #open image
  layer = resnet._modules.get('avgpool')
  img = Image.open(img_name)

  #make image 224*224
  #convert to pytorch tensor

  preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )])

  input_tensor = preprocess(img)
  input_batch = input_tensor.unsqueeze(0)

  with torch.no_grad():
      my_output = None
      
      def my_hook(module_, input_, output_):
          nonlocal my_output
          my_output = output_

      a_hook = resnet.avgpool.register_forward_hook(my_hook)        
      resnet(input_batch)
      a_hook.remove()
      return my_output

get_feature_vector('/content/drive/MyDrive/small_recipe_dataset/1/2/a/b/12ab235.jpg')

def get_features(data_dir):
  feature_vectors = {}
  #load recipe dataset

  transform = transforms.Compose([
          transforms.Resize(256),
          transforms.CenterCrop(224),
          transforms.ToTensor(),
          transforms.Normalize(
              mean=[0.485, 0.456, 0.406],
          std=[0.229, 0.224, 0.225]
      )])

  image_data = datasets.ImageFolder(data_dir, transform=transform) # TODO: create the ImageFolder
  print(image_data.imgs)
  data_loader = torch.utils.data.DataLoader(image_data, batch_size = 256, shuffle=False) # TODO: use the ImageFolder dataset to create the DataLoader
  batch_num = 1
  with torch.no_grad():
    for images, labels in data_loader:
      #device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
      #images, labels = images.to(device), labels.to(device)
      #print("Loop")
      #print(images.shape)
      #print(images)
      #print(labels)
      #print(labels)
      #print("Batch num = ", batch_num)
      resnet.fc = nn.Identity()
      feature_vectors[batch_num] = resnet(images)
      batch_num += 1
      #id_to_feature = dict(zip(image_data.imgs[], feature_vectors))
  return feature_vectors

#data_dir = '/content/drive/MyDrive/recipe_images'
data_dir = '/content/drive/MyDrive/small_recipe_dataset'
feature_vectors1 = get_features(data_dir)

feature_vectors1

#data_dir = '/content/drive/MyDrive/recipe_images'
data_dir = '/content/drive/MyDrive/small_recipe_dataset/4'
feature_vectors2 = get_features(data_dir)

for i in range(4):
  print(feature_vectors2[1][i])

from itertools import islice
with open('check.txt') as f:
    while True:
        next_n_lines = list(islice(f, 11))
        if not next_n_lines:
            break

        print(next_n_lines)
        # process next_n_lines

print(feature_vectors.shape)
#zip with ids
#id_to_feature = dict(zip(, feature_vectors))
print(type(feature_vectors[0]))
print(feature_vectors[0])
print((feature_vectors[0].data).cpu().numpy())

# image_ids
# for the entire folder we get feature_vectors
# the ids are in the file, but we dont know their order
# Why don't we just read file names from the folder
!ls /content/drive/MyDrive/small_recipe_dataset
!ls | wc -l
#image_ids = []
#id_to_feature = dict(zip(feature_vectors, feature_vectors.numpy()))

import numpy as np
import pickle

def save_feature_vectors_to_pkl(feature_vectors):
  data_map = {}
  data_map['batch1'] = feature_vectors

  with open('test.pkl','wb') as f:
    pickle.dump(data_map, f)

def read_from_pkl():
  with open('test.pkl','rb') as f:
    x = pickle.load(f)
    print(x['batch1'].shape)
    #print(x.shape)


def loadFromPickle():
  with open('test.pkl', "rb") as f:
    while True:
      try:
        yield pickle.load(f)
      except EOFError:
        break

#loaded = loadFromPickle()
#saving all the feature vectors in the pickle
#save_feature_vectors_to_pkl(feature_vectors)

read_from_pkl()

with open('test.pkl','rb') as f:
  x = pickle.load(f)
  print(x)

#How many .pkl files to create?
# Will one be enough? It is possible, but we are not loading the entire dataset in one go. We are loading it in batches
# Batches of 512. So that many images will be loaded
# If we do the batching, then does that mean it implicitly will only load the batch data in the memory
# Thats how it should be
# But we do not have the full data on colab
# But on Jupyter if we load by batch
# Now we have another option, the IterableDataset, this wont load the whole thing in memory
# Lets try with that
# lets just try with 1 .pkl file
# And how to store, a single big dictionary, with key = id, and value = feature_vector
# we get a vector of tensors
# how to map that?
# The map function? How to zip these two lists in a dict

a = [1,2,3]
b = [4,5,6]
new_dict = dict(zip(a, b))

new_dict

"""Total 887536 images in the layer2.json
How many are unique?
All 887536 are unique. Or at least they have unique ids
"""

#def read_from_pkl():
with open('/content/drive/MyDrive/Recipe1M_features/embeddings_train1.pkl','rb') as f:
  x = pickle.load(f)
  #print(x.shape)

#read_from_pkl()

x[2][0]

x[2].shape

"""# CCA"""

from sklearn.cross_decomposition import CCA

#X = [[0., 0., 1.], [1.,0.,0.], [2.,2.,2.], [3.,5.,4.]]
#Y = [[0.1, -0.2], [0.9, 1.1], [6.2, 5.9], [11.9, 12.3]]
X = x[0]
cca = CCA(n_components=1)
cca.fit(X, Y)
CCA(n_components=1)
X_c, Y_c = cca.transform(X, Y)

#text_embeddings = pd.read_csv('/content/drive/MyDrive/ml/final_bert_encodings.csv',  encoding='ISO-8859–1', )
#text_embeddings = pd.read_csv('/content/drive/MyDrive/ml/final_bert_encodings.csv',  encoding='ISO-8859–1', sep='delimiter', header=None)

chunksize = 10 ** 3
df = pd.DataFrame()

with pd.read_csv('/content/drive/MyDrive/ml/final_bert_encodings.csv', compression = 'gzip', chunksize=chunksize, header=None) as reader:
    for chunk in reader:
        df = pd.concat([df, chunk])

df.head()